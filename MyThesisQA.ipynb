{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ask my PhD. thesis a question**"
      ],
      "metadata": {
        "id": "I4hXN0IvQS4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Necessary pachages"
      ],
      "metadata": {
        "id": "w73hxFgeQTKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain\n",
        "!pip install pypdf pymupdf pdfplumber pypdf2\n",
        "!pip install google-generativeai\n",
        "!pip install chromadb\n",
        "!pip install python-dotenv\n",
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "id": "rTV2_HiEQVEm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load necessary libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Langchain and Generative AI imports\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Google Generative AI import\n",
        "import google.generativeai as genai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yHXMBMLLPVQT"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Google drive to read the thesis pdf and import environment variables"
      ],
      "metadata": {
        "id": "cUqtn-DpLi2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWSpX_rXLiVO",
        "outputId": "30f98152-2270-4db0-abfd-831fba3ee90c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If environment keys (API_key) is not set up, set it up here!"
      ],
      "metadata": {
        "id": "DpNZled6MFSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_path = '/content/drive/My Drive/.env'\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv(env_path)\n",
        "\n",
        "# Access the environment variables\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "# Print the variable to verify\n",
        "#print(f'GOOGLE_API_KEY: {google_api_key}')"
      ],
      "metadata": {
        "id": "oly2FTlKL4ku"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key = google_api_key)"
      ],
      "metadata": {
        "id": "wA7HewiTPNrG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for m in genai.list_models():\n",
        " # print(m)"
      ],
      "metadata": {
        "id": "zgtHp7mTPdvS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "f3c8ad32-543c-4acb-b2cb-d7ffc084791c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 Chat (Legacy)',\n",
            "      description='A legacy text-only model optimized for chat conversations',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 (Legacy)',\n",
            "      description='A legacy model that understands text and generates text as an output',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
            "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
            "                   'model that supports tuning.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Latest',\n",
            "      description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
            "                   'model.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description='The best image understanding model to handle a broad range of applications',\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash Latest',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro',\n",
            "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro 001',\n",
            "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro Latest',\n",
            "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description='The best image understanding model to handle a broad range of applications',\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load my thesis document"
      ],
      "metadata": {
        "id": "je6NqMmKtrp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load the PDF\n",
        "pdf_loader = PyPDFLoader(\"/content/drive/My Drive/dissertation/Dalilian_Disseration_QA.pdf\")\n",
        "documents = pdf_loader.load()\n"
      ],
      "metadata": {
        "id": "ijsLuTpQ2God"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[100:102]"
      ],
      "metadata": {
        "id": "FMvlD8uKM2BM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e243c76-4a9c-4e7b-e97d-57a8fc136232"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/drive/My Drive/dissertation/Dalilian_Disseration_QA.pdf', 'page': 100}, page_content=\"   \\n \\n87   \\nTable 8: The range of hyperparameters for the SVM model considered . \\nHyperparameter  Range  \\nFeature Calculation Window Length  2, 3, 4, 5, 6, 7, 8, 9 , 10 \\nKernel Function  Linear, RBF  \\nRegularization Parameter 'C'  0.001, 0.005,0.1,0.05,0.01,0.5,1  \\nGamma (for RBF Kernels)  0.001, 0.01, 0.1, 1  \\n \\nWe tested linear and Radial Basis Function (RBF) kernels to identify a hyperplane \\n(decision boundary) that effectively separates the data by class. Kernels transform the data into a \\nhigher -dimensional space, where the linear kernel seeks a direct linear separation, and the RBF \\nkernel, through its non -linear mapping, aims to find a more complex boundary (Han et al., 2012) . \\nThe RBF kernel is characterized by a gamma parameter that controls the shape of the decision \\nboundary. To find a trade -off between maximizing the margin between classes and minimizing \\nclassification errors, a range of regularization ‚ÄòC‚Äô parameters  were tested.  \\nA grid search was conducted to examine all hyperparameter combinations within the \\nspecified range. This search thoroughly explored all hyperparameter combinations, aiming to \\nenhance the average Matthews Correlation Coefficient (MCC). The choice of using MC C score \\nfor hyperparameter tuning was due to the notable data imbalance indicated by the 11% ratio of \\nmissed to detected cracks. We note that as a practical matter, such a ratio represents the realistic \\naspect of inspection regimes with many more hits than  misses. As MCC takes into account \\naccuracies and error rates on both classes, and involves all confusion matrix values, it a \\nparticularly effective metric for evaluating models in scenarios of imbalanced datasets (Bekkar et \\nal., 2013) . The formula for calculating MCC is as follows:   \"),\n",
              " Document(metadata={'source': '/content/drive/My Drive/dissertation/Dalilian_Disseration_QA.pdf', 'page': 101}, page_content=\"   \\n \\n88  ùëÄùê∂ùê∂ = (ùëáùëÉ √óùëáùëÅ)‚àí(ùêπùëÉ√óùêπùëÅ)\\n‚àö(ùëáùëÉ+ùêπùëÉ)(ùëáùëÉ+ùêπùëÅ)(ùëáùëÅ+ùêπùëÉ)(ùëáùëÅ+ùêπùëÅ) \\nTP is the number of true positives,  TN is the number of true negatives,  FP is the number \\nof false positives,  and FN is the number of false negatives.  MCC spans the range from 1, \\nrepresenting a perfect prediction, to -1, for perfectly incorrect prediction . An MCC value near 0 \\nsuggests prediction s similar  to random chance.  \\nAs outlined in the cross -validation section, Stratified Group 5-Fold Cross -Validation was \\nused during the hyperparameter tuning process . This method was chosen to ensure that the model \\ncould identify features that were not dependent on individual participants. It also allowed for the \\nevaluation of the model's performance across various groups of participants. The process \\ninvolved holding out one fold for validation while the remaining four folds are used for training \\nthe model. This is repeated five times, with each fold being used as the validation set exactly \\nonce and as part of the training set four times. A fter each fold has served as th e validation set, the \\nperformance metrics from all five iterations are averaged to provide an overall assessment of the \\nmodel's performance. By using this approach, it was possible to select hyperparameters that \\nconsistently delivered the best outcomes.   \\n4.2.4. Statistical Significance of SVM Model  \\nTo validate the performance of our best model beyond mere random chance, a one -sample t -test \\nwas conducted . This test was applied to compare the maximum Matthews Correlation \\nCoefficient (MCC) obtained by our model against a theoretical average MCC of 0, which \\nrepresents the expected outcome of random guessing. This comparison was crucial for \\nestablishing the s tatistical significance of our model's performance.  \")]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Split the document into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Number of chunks: {len(chunks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7R0EQYG2HCE",
        "outputId": "fb75a686-8244-4192-d20d-56da0f7b6bf9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create embeddings of the chuncks of my thesis to get ready for semantic search**\n",
        "\n",
        "Here we prepare chunchs for similarity searches. This is done through embedding our chunks of text (getting a vector per chunk).\n"
      ],
      "metadata": {
        "id": "RFxaBXPcNv-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [doc.page_content for doc in chunks]\n"
      ],
      "metadata": {
        "id": "aamg6mdl30Oi"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J0Vfw0KQthnB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = FAISS.from_texts(texts, embedding=embeddings)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yIDnjilmqUOP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(query):\n",
        "  similar_docs = vector_store.similarity_search(query)\n",
        "  llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",\n",
        "                             temperature=0.3)\n",
        "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "  return chain.run(input_documents=similar_docs, question=query)"
      ],
      "metadata": {
        "id": "Zwx9jaccML53"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"how cognitive monitoring is useful for human-robot collaboration?\"\n",
        "answer_question(query1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ZFk3YUKfMhwS",
        "outputId": "00aad1bd-f6d2-40c7-c770-3ffc3651c145"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Cognitive monitoring is useful for human-robot collaboration in two main ways:\\n\\n1. **Identifying Suboptimal States:** It helps identify when human collaborators are experiencing suboptimal states like stress, fatigue, or lack of attention. This allows for timely interventions, such as suggesting breaks, adjusting tasks, or prompting the human to re-engage. This ensures the well-being and safety of the human operator.\\n\\n2. **Enabling Adaptive Automation:**  It provides feedback to the robotic system about the human's cognitive state. This allows the robot to adapt its operation dynamically, such as adjusting the pace of work, modifying task assignments, or activating safety protocols. This leads to a more efficient and empathetic human-robot interaction where the machine is responsive to the human's condition. \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    }
  ]
}